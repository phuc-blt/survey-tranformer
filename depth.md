### Depth
- (arXiv 2020.11) Revisiting Stereo Depth Estimation From a Sequence-to-Sequence Perspective with Transformers, [[Paper]](https://arxiv.org/pdf/2011.02910.pdf), [[Code]](https://github.com/mli0603/stereo-transformer)
- (arXiv 2021.03) Vision Transformers for Dense Prediction, [[Paper]](https://arxiv.org/pdf/2103.13413.pdf), [[Code]](https://github.com/intel-isl/DPT)
- (arXiv 2021.03) Transformers Solve the Limited Receptive Field for Monocular Depth Prediction, [[Paper]](https://arxiv.org/pdf/2103.12091.pdf), [[Code]](https://github.com/ygjwd12345/TransDepth)
- (arXiv 2021.09) Improving 360 Monocular Depth Estimation via Non-local Dense Prediction Transformer and Joint Supervised and Self-supervised Learning, [[Paper]](https://arxiv.org/pdf/2109.10563.pdf)
- (arXiv 2022.02) GLPanoDepth: Global-to-Local Panoramic Depth Estimation, [[Paper]](https://arxiv.org/pdf/2202.02796.pdf)
- (arXiv 2022.02) Transformers in Self-Supervised Monocular Depth Estimation with Unknown Camera Intrinsics, [[Paper]](https://arxiv.org/pdf/2202.03131.pdf)
- (arXiv 2022.03) OmniFusion: 360 Monocular Depth Estimation via Geometry-Aware Fusion, [[Paper]](https://arxiv.org/pdf/2203.00838.pdf)
- (arXiv 2022.03) PanoFormer: Panorama Transformer for Indoor 360 Depth Estimation, [[Paper]](https://arxiv.org/pdf/2203.09283.pdf)
- (arXiv 2022.03) DepthGAN: GAN-based Depth Generation of Indoor Scenes from Semantic Layouts, [[Paper]](https://arxiv.org/pdf/2203.11453.pdf)
- (arXiv 2022.03) DepthFormer: Exploiting Long-Range Correlation and Local Information for Accurate Monocular Depth Estimation, [[Paper]](https://arxiv.org/pdf/2203.14211.pdf), [[Code]](https://github.com/zhyever/Monocular-Depth-Estimation-Toolbox)
- (arXiv 2022.04) BinsFormer: Revisiting Adaptive Bins for Monocular Depth Estimation, [[Paper]](https://arxiv.org/pdf/2204.00987.pdf), [[Code]](https://github.com/zhyever/Monocular-Depth-Estimation-Toolbox)
- (arXiv 2022.04) SurroundDepth: Entangling Surrounding Views for Self-Supervised Multi-Camera Depth Estimation, [[Paper]](https://arxiv.org/pdf/2204.03636.pdf), [[Project]](https://surrounddepth.ivg-research.xyz/)
- (arXiv 2022.04) Multi-Frame Self-Supervised Depth with Transformers, [[Paper]](https://arxiv.org/pdf/2204.07616.pdf), [[Project]](https://sites.google.com/tri.global/depthformer)
- (arXiv 2022.05) SideRT: A Real-time Pure Transformer Architecture for Single Image Depth Estimation, [[Paper]](https://arxiv.org/pdf/2204.13892.pdf)
- (arXiv 2022.05) Depth Estimation with Simplified Transformer, [[Paper]](https://arxiv.org/pdf/2204.13791.pdf)
- (arXiv 2022.05) MonoFormer: Towards Generalization of self-supervised monocular depth estimation with Transformers, [[Paper]](https://arxiv.org/pdf/2205.11083.pdf)
- (arXiv 2022.06) SparseFormer: Attention-based Depth Completion Network, [[Paper]](https://arxiv.org/pdf/2206.04557.pdf)
- (arXiv 2022.06) Forecasting of depth and ego-motion with transformers and self-supervision, [[Paper]](https://arxiv.org/pdf/2206.07435.pdf)
- (arXiv 2022.07) Depthformer : Multiscale Vision Transformer For Monocular Depth Estimation With Local Global Information Fusion, [[Paper]](https://arxiv.org/pdf/2207.04535.pdf), [[Code]](https://github.com/ashutosh1807/Depthformer.git)
- (arXiv 2022.08) MonoViT: Self-Supervised Monocular Depth Estimation with a Vision Transformer, [[Paper]](https://arxiv.org/pdf/2208.03543.pdf), [[Code]](https://github.com/zxcqlf/MonoViT)
- (arXiv 2022.09) TODE-Trans: Transparent Object Depth Estimation with Transformer, [[Paper]](https://arxiv.org/pdf/2209.08455.pdf), [[Code]](https://github.com/yuchendoudou/TODE)
- (arXiv 2022.10) Context-Enhanced Stereo Transformer, [[Paper]](https://arxiv.org/pdf/2210.11719.pdf), [[Code]](https://github.com/guoweiyu/Context-Enhanced-Stereo-Transformer)
- (arXiv 2022.11) Hybrid Transformer Based Feature Fusion for Self-Supervised Monocular Depth Estimation, [[Paper]](https://arxiv.org/pdf/2211.11066.pdf)
- (arXiv 2022.11) Lite-Mono: A Lightweight CNN and Transformer Architecture for Self-Supervised Monocular Depth Estimation, [[Paper]](https://arxiv.org/pdf/2211.13202.pdf), [[Code]](https://github.com/noahzn/lite-mono)
- (arXiv 2022.12) Event-based Monocular Dense Depth Estimation with Recurrent Transformers, [[Paper]](https://arxiv.org/pdf/2212.02791.pdf)
- (arXiv 2022.12) ROIFormer: Semantic-Aware Region of Interest Transformer for Efficient Self-Supervised Monocular Depth Estimation, [[Paper]](https://arxiv.org/pdf/2212.05729.pdf)
- (arXiv 2023.01) Dyna-DepthFormer: Multi-frame Transformer for Self-Supervised Depth Estimation in Dynamic Scenes, [[Paper]](https://arxiv.org/pdf/2301.05871.pdf)
- (arXiv 2023.01) SwinDepth: Unsupervised Depth Estimation using Monocular Sequences via Swin Transformer and Densely Cascaded Network, [[Paper]](https://arxiv.org/pdf/2301.06715.pdf)
- (arXiv 2023.02) URCDC-Depth: Uncertainty Rectified Cross-Distillation with CutFlip for Monocular Depth Estimation, [[Paper]](https://arxiv.org/pdf/2302.08149.pdf), [[Code]](https://github.com/ShuweiShao/URCDC-Depth)
- (arXiv 2023.03) STDepthFormer: Predicting Spatio-temporal Depth from Video with a Self-supervised Transformer Model, [[Paper]](https://arxiv.org/pdf/2303.01196.pdf), [[Code]](https://github.com/ShuweiShao/URCDC-Depth)
- (arXiv 2023.03) DwinFormer: Dual Window Transformers for End-to-End Monocular Depth Estimation, [[Paper]](https://arxiv.org/pdf/2303.02968.pdf)
- (arXiv 2023.03) DEHRFormer: Real-time Transformer for Depth Estimation and Haze Removal from Varicolored Haze Scenes, [[Paper]](https://arxiv.org/pdf/2303.06905.pdf)
- (arXiv 2023.03) Channel-Aware Distillation Transformer for Depth Estimation on Nano Drones, [[Paper]](https://arxiv.org/pdf/2303.10386.pdf)
- (arXiv 2023.04) EGformer: Equirectangular Geometry-biased Transformer for 360 Depth Estimation, [[Paper]](https://arxiv.org/pdf/2304.07803.pdf)
- (arXiv 2023.04) CompletionFormer: Depth Completion with Convolutions and Vision Transformers, [[Paper]](https://arxiv.org/pdf/2304.13030.pdf), [[Code]](https://github.com/youmi-zym/CompletionFormer)
- (arXiv 2023.08) Improving Depth Gradient Continuity in Transformers: A Comparative Study on Monocular Depth Estimation with CNN, [[Paper]](https://arxiv.org/pdf/2308.08333.pdf)
- (arXiv 2023.08) Semi-Supervised Semantic Depth Estimation using Symbiotic Transformer and NearFarMix Augmentation, [[Paper]](https://arxiv.org/pdf/2308.14400.pdf)
- (arXiv 2023.09) SQLdepth: Generalizable Self-Supervised Fine-Structured Monocular Depth Estimation, [[Paper]](https://arxiv.org/pdf/2309.00526.pdf), [[Code]](https://github.com/hisfog/SQLdepth-Impl)
- (arXiv 2023.10) GSDC Transformer: An Efficient and Effective Cue Fusion for Monocular Multi-Frame Depth Estimation, [[Paper]](https://arxiv.org/pdf/2309.17059.pdf)
- (arXiv 2023.10) FocDepthFormer: Transformer with LSTM for Depth Estimation from Focus, [[Paper]](https://arxiv.org/pdf/2309.17059.pdf)
- (arXiv 2023.10) Metrically Scaled Monocular Depth Estimation through Sparse Priors for Underwater Robots, [[Paper]](https://arxiv.org/pdf/2310.16750.pdf), [[Code]](https://github.com/ebnerluca/uw_depth)
- (arXiv 2023.12) Transformers in Unsupervised Structure-from-Motion, [[Paper]](https://arxiv.org/pdf/2312.10529.pdf), [[Code]](https://github.com/NeurAI-Lab/MT-SfMLearner)
- (arXiv.2024.02) CLIP Can Understand Depth, [[Paper]](https://arxiv.org/pdf/2402.03251.pdf)
- (arXiv.2024.03) Depth Estimation Algorithm Based on Transformer-Encoder and Feature Fusion, [[Paper]](https://arxiv.org/pdf/2403.01370.pdf)
- (arXiv.2024.03) METER: a mobile vision transformer architecture for monocular depth estimation, [[Paper]](https://arxiv.org/pdf/2403.08368.pdf)
- (arXiv.2024.03) SSAP: A Shape-Sensitive Adversarial Patch for Comprehensive Disruption of Monocular Depth Estimation in Autonomous Navigation Applications, [[Paper]](https://arxiv.org/pdf/2403.11515.pdf)
- (arXiv.2024.03) UniDepth: Universal Monocular Metric Depth Estimation, [[Paper]](https://arxiv.org/pdf/2403.18913.pdf), [[Code]](https://github.com/lpiccinelli-eth/unidepth)
- (arXiv.2024.04) WorDepth: Variational Language Prior for Monocular Depth Estimation, [[Paper]](https://arxiv.org/pdf/2404.03635.pdf), [[Code]](https://github.com/Adonis-galaxy/WorDepth)
- (arXiv.2024.04) SGFormer: Spherical Geometry Transformer for 360 Depth Estimation, [[Paper]](https://arxiv.org/pdf/2404.14979.pdf)
- (arXiv.2024.06) ToSA: Token Selective Attention for Efficient Vision Transformers, [[Paper]](https://arxiv.org/pdf/2406.08816.pdf)
- (arXiv.2024.07) Towards Scale-Aware Full Surround Monodepth with Transformers, [[Paper]](https://arxiv.org/pdf/2407.10406.pdf)
- (arXiv.2024.07) UMono: Physical Model Informed Hybrid CNN-Transformer Framework for Underwater Monocular Depth Estimation, [[Paper]](https://arxiv.org/pdf/2407.17838.pdf)
- (arXiv.2024.09) SDformer: Efficient End-to-End Transformer for Depth Completion, [[Paper]](https://arxiv.org/pdf/2409.08159.pdf), [[Code]](https://github.com/JamesQian11/SDformer-for-Depth-Completion)
- (arXiv.2024.09) Depth Matters: Exploring Deep Interactions of RGB-D for Semantic Segmentation in Traffic Scenes, [[Paper]](https://arxiv.org/pdf/2409.07995.pdf)
