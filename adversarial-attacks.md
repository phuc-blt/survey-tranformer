### Adversarial Attacks
- (arXiv 2022.06) Exploring Adversarial Attacks and Defenses in Vision Transformers trained with DINO, [[Paper]](https://arxiv.org/pdf/2206.06761.pdf), [[Code]](https://github.com/thobauma/AADefDINO)
- (arXiv 2022.06) Backdoor Attacks on Vision Transformers, [[Paper]](https://arxiv.org/pdf/2206.08477.pdf), [[Code]](https://github.com/UCDvision/backdoor_transformer.git)
- (arXiv 2022.06) Defending Backdoor Attacks on Vision Transformer via Patch Processing, [[Paper]](https://arxiv.org/pdf/2206.12381.pdf)
- (arXiv 2022.07) Towards Efficient Adversarial Training on Vision Transformers, [[Paper]](https://arxiv.org/pdf/2207.10498.pdf)
- (arXiv 2022.08) Understanding Adversarial Robustness of Vision Transformers via Cauchy Problem, [[Paper]](https://arxiv.org/pdf/2208.00906.pdf), [[Code]](https://github.com/TrustAI/ODE4RobustViT)
- (arXiv 2022.08) Analyzing Adversarial Robustness of Vision Transformers against Spatial and Spectral Attacks, [[Paper]](https://arxiv.org/pdf/2208.09602.pdf)
- (arXiv 2023.01) Inference Time Evidences of Adversarial Attacks for Forensic on Transformers, [[Paper]](https://arxiv.org/pdf/2301.13356.pdf)
- (arXiv 2023.03) Transferable Adversarial Attacks on Vision Transformers with Token Gradient Regularization, [[Paper]](https://arxiv.org/pdf/2303.15754.pdf)
- (arXiv 2023.05) On enhancing the robustness of Vision Transformers: Defensive Diffusion, [[Paper]](https://arxiv.org/pdf/2305.08031.pdf), [[Code]](https://github.com/Muhammad-Huzaifaa/Defensive_Diffusion)
- (arXiv 2023.06) Pre-trained transformer for adversarial purification, [[Paper]](https://arxiv.org/pdf/2306.01762.pdf)
- (arXiv 2023.07) Random Position Adversarial Patch for Vision Transformers, [[Paper]](https://arxiv.org/pdf/2307.04066.pdf)
- (arXiv 2023.07) Enhanced Security against Adversarial Examples Using a Random Ensemble of Encrypted Vision Transformer Models, [[Paper]](https://arxiv.org/pdf/2307.13985.pdf)
- (arXiv 2023.09) Exploring Non-additive Randomness on ViT against Query-Based Black-Box Attacks, [[Paper]](https://arxiv.org/pdf/2309.06438.pdf)
- (arXiv 2023.09) RBFormer: Improve Adversarial Robustness of Transformer by Robust Bias, [[Paper]](https://arxiv.org/pdf/2309.13245.pdf)
- (arXiv 2023.10) Multimodal Adversarial Attacks on Vision-Language Tasks via Pre-trained Models, [[Paper]](https://arxiv.org/pdf/2310.04655.pdf)
- (arXiv 2023.10) ConViViT -- A Deep Neural Network Combining Convolutions and Factorized Self-Attention for Human Activity Recognition, [[Paper]](https://arxiv.org/pdf/2310.14416.pdf)
- (arXiv 2023.10) Blacksmith: Fast Adversarial Training of Vision Transformers via a Mixture of Single-step and Multi-step Methods, [[Paper]](https://arxiv.org/pdf/2310.18975.pdf)
- (arXiv 2023.11) DialMAT: Dialogue-Enabled Transformer with Moment-Based Adversarial Training, [[Paper]](https://arxiv.org/pdf/2311.06855.pdf)
- (arXiv 2023.11) Attention Deficit is Ordered! Fooling Deformable Vision Transformers with Collaborative Adversarial Patches, [[Paper]](https://arxiv.org/pdf/2311.12914.pdf)
- (arXiv 2023.12) MIMIR: Masked Image Modeling for Mutual Information-based Adversarial Robustness, [[Paper]](https://arxiv.org/pdf/2312.04960.pdf), [[Code]](https://github.com/xiaoyunxxy/MIMIR)
- (arXiv 2024.01) FullLoRA-AT: Efficiently Boosting the Robustness of Pretrained Vision Transformers, [[Paper]](https://arxiv.org/pdf/2401.01752.pdf)
- (arXiv 2024.02) DeSparsify: Adversarial Attack Against Token Sparsification Mechanisms in Vision Transformers, [[Paper]](https://arxiv.org/pdf/2402.02554.pdf)
- (arXiv 2024.03) Attacking Transformers with Feature Diversity Adversarial Perturbation, [[Paper]](https://arxiv.org/pdf/2403.07942.pdf)
- (arXiv 2024.03) Approximate Nullspace Augmented Finetuning for Robust Vision Transformers, [[Paper]](https://arxiv.org/pdf/2403.10476.pdf)
- (arXiv 2024.05) Not All Prompts Are Secure: A Switchable Backdoor Attack Against Pre-trained Vision Transformers, [[Paper]](https://arxiv.org/pdf/2405.10612.pdf),[[Code]](https://github.com/20000yshust/SWARM)
- (arXiv 2024.07) Query-Efficient Hard-Label Black-Box Attack against Vision Transformers, [[Paper]](https://arxiv.org/pdf/2407.00389.pdf)
- (arXiv 2024.07) TrackPGD: A White-box Attack using Binary Masks against Robust Transformer Trackers, [[Paper]](https://arxiv.org/pdf/2407.03946.pdf)
- (arXiv 2024.07) S-E Pipeline: A Vision Transformer (ViT) based Resilient Classification Pipeline for Medical Imaging Against Adversarial Attacks, [[Paper]](https://arxiv.org/pdf/2407.17587.pdf)
- (arXiv 2024.08) Downstream Transfer Attack: Adversarial Attacks on Downstream Models with Pre-trained Vision Transformers, [[Paper]](https://arxiv.org/pdf/2408.01705.pdf)
- (arXiv 2024.09) ViTGuard: Attention-aware Detection against Adversarial Examples for Vision Transformer, [[Paper]](https://arxiv.org/pdf/2409.13828)
- (arXiv 2024.09) RoWSFormer: A Robust Watermarking Framework with Swin Transformer for Enhanced Geometric Attack Resilience, [[Paper]](https://arxiv.org/pdf/2409.14829)
